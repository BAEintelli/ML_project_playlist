{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multilabel_classifier로 val playlist를 채워보자\n",
    "## 1. val의 song을 기준으로 tag를 10개씩 채우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json('datas/train.json')\n",
    "val = pd.read_json('datas/val.json')\n",
    "song_meta = pd.read_json('datas/song_meta.json')\n",
    "raw_df = pd.read_csv('datas/df_for_MLC.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. fit, predict를 위한 train + val Dataframe 만들기\n",
    "#### 연도의 원핫인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T11:15:48.068935Z",
     "iopub.status.busy": "2020-05-28T11:15:48.066935Z",
     "iopub.status.idle": "2020-05-28T11:15:48.273393Z",
     "shell.execute_reply": "2020-05-28T11:15:48.272352Z",
     "shell.execute_reply.started": "2020-05-28T11:15:48.068935Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'raw_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-6f87c212dc2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mraw_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'issue_year'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'issue_month'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mraw_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'issue_year'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'issue_month'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfinal_df_year_month_dummies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'issue_year'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'issue_month'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfinal_df_year_month_dummies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfinal_df_year_month_dummies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfinal_df_one_hot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mraw_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mraw_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'issue_year'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'issue_month'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_df_year_month_dummies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'raw_df' is not defined"
     ]
    }
   ],
   "source": [
    "raw_df[['issue_year','issue_month']] = raw_df[['issue_year','issue_month']].apply(lambda x : [str(i) for i in x])\n",
    "final_df_year_month_dummies = pd.get_dummies(raw_df[['issue_year','issue_month']])\n",
    "final_df_year_month_dummies.reset_index(inplace=True)\n",
    "final_df_year_month_dummies.rename(columns={'index':'id'},inplace=True)\n",
    "final_df_one_hot = raw_df[raw_df.columns.difference(['issue_year','issue_month'])].merge(final_df_year_month_dummies, on='id')\n",
    "final_df_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### id 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_no_id = final_df_one_hot[final_df_one_hot.columns.difference(['id'])]\n",
    "final_df_no_id[['population','like_cnt','tag_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "MinMax_scaled = minmax_scale(final_df_no_id, axis=0, copy=True, )\n",
    "df_for_KNN = pd.DataFrame(MinMax_scaled, columns = final_df_no_id.columns)\n",
    "df_for_KNN['id'] = final_df_one_hot['id']\n",
    "df_for_KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train과 val 데이터 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "df_songs = train['songs'].apply(lambda x : [str(i) for i in x])\n",
    "df_songs = train['songs'].reset_index(drop=True)\n",
    "unique_song_list = np.concatenate(df_songs)\n",
    "unique_song_list = list(set(unique_song_list))\n",
    "unique_song_list = [int(i) for i in unique_song_list]\n",
    "df_song_train = df_for_KNN[df_for_KNN['id'].isin(unique_song_list)]\n",
    "df_song_train.drop(columns = 'Unnamed: 0', inplace = True)\n",
    "df_song_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val data\n",
    "val_songs = val['songs'].apply(lambda x : [str(i) for i in x])\n",
    "val_songs = val['songs'].reset_index(drop=True)\n",
    "unique_song_list = np.concatenate(val_songs)\n",
    "unique_song_list = list(set(unique_song_list))\n",
    "unique_song_list = [int(i) for i in unique_song_list]\n",
    "df_song_val = df_for_KNN[df_for_KNN['id'].isin(unique_song_list)]\n",
    "df_song_val.drop(columns = 'Unnamed: 0', inplace = True)\n",
    "df_song_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trian에 y(tag) 붙이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = []\n",
    "for i in train[['tags','songs']].index:\n",
    "#     tags = train.get_value(i,'tags')\n",
    "#     songs = train.get_value(i,'songs')\n",
    "    tags = train.at[i,'tags']\n",
    "    songs = train.at[i,'songs']\n",
    "    for song in songs:\n",
    "        ls.append({'id':song,'tags': tags})\n",
    "song_tag = pd.DataFrame(ls)\n",
    "song_tag_df = song_tag.groupby('id').agg(sum).reset_index()\n",
    "song_tag_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_song_train = df_song_train.merge(song_tag_df,on='id')\n",
    "df_song_train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 피클파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_song_train.set_index('id', inplace = True)\n",
    "df_song_val.set_index('id', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T06:15:50.088631Z",
     "iopub.status.busy": "2020-05-28T06:15:50.088631Z",
     "iopub.status.idle": "2020-05-28T06:15:50.093617Z",
     "shell.execute_reply": "2020-05-28T06:15:50.092630Z",
     "shell.execute_reply.started": "2020-05-28T06:15:50.088631Z"
    }
   },
   "source": [
    "#### 불러오기 (확인용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_song_train.to_pickle('datas/df_song_train.pkl')\n",
    "df_song_val.to_pickle('datas/df_song_val.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. keras model 생성 \n",
    "- song -> tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T07:20:42.578461Z",
     "iopub.status.busy": "2020-05-28T07:20:42.577464Z",
     "iopub.status.idle": "2020-05-28T07:20:49.783716Z",
     "shell.execute_reply": "2020-05-28T07:20:49.783716Z",
     "shell.execute_reply.started": "2020-05-28T07:20:42.578461Z"
    }
   },
   "outputs": [],
   "source": [
    "df_song_train = pd.read_pickle('datas/df_song_train.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train에 y를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T07:20:49.784716Z",
     "iopub.status.busy": "2020-05-28T07:20:49.784716Z",
     "iopub.status.idle": "2020-05-28T07:20:55.935552Z",
     "shell.execute_reply": "2020-05-28T07:20:55.934552Z",
     "shell.execute_reply.started": "2020-05-28T07:20:49.784716Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "raw_te_tags_mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "raw_te_tags = list(df_song_train.tags)\n",
    "y = raw_te_tags_mlb.fit_transform(raw_te_tags)\n",
    "y_classes = raw_te_tags_mlb.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train의 X를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T07:20:57.638030Z",
     "iopub.status.busy": "2020-05-28T07:20:57.638030Z",
     "iopub.status.idle": "2020-05-28T07:20:58.442842Z",
     "shell.execute_reply": "2020-05-28T07:20:58.442842Z",
     "shell.execute_reply.started": "2020-05-28T07:20:57.638030Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df_song_train[df_song_train.columns.difference(['tags'])]\n",
    "X = X.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train의 song으로 tag를 예측하는 모델 생성\n",
    "- multi_tag_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T07:20:58.443885Z",
     "iopub.status.busy": "2020-05-28T07:20:58.443885Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 생성 및 하이퍼 파라미터 튜닝\n",
    "import os, glob, numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import BatchNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend.tensorflow_backend as K\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "with K.tf_ops.device('/device:CPU:0'):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(32, input_shape=X.shape[1:], activation='relu'))\n",
    "    \n",
    "    model.add(Dense(64, activation='relu'))\n",
    "\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(y.shape[1], activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam')\n",
    "    \n",
    "    model_dir = './model'\n",
    "\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "\n",
    "    model_path = model_dir + '/multi_tag_classification.h5'\n",
    "    checkpoint = ModelCheckpoint(filepath=model_path , monitor='loss', verbose=1, save_weights_only=True, period=5)\n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치사이즈, 에포크, 검증 데이터 사이즈 수정이 필요해보임\n",
    "history = model.fit(X, y, batch_size=1000, epochs=10, validation_split = 0.2, callbacks=[checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('predict_tags_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모델 저장(1) -> 모델의 껍데기 fit되지않은애가 저장됨.\n",
    "# model_json = model.to_json()\n",
    "# with open(\"model/multi_tag_classification.json\", \"w\") as json_file : \n",
    "#     json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델 불러오기(1) -> 모델의 껍데기 fit되지 않은애가 불러와짐\n",
    "# from keras.models import model_from_json \n",
    "# # json_file = open(\"model/multi_tag_classification.json\", \"r\") \n",
    "# loaded_model_json = json_file.read() \n",
    "# json_file.close() \n",
    "# model = model_from_json(loaded_model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 가중치 불러오기(가중치만 저장됐을떄) -> 'multi_tag_classification.h5'\n",
    "# 위의 모델 정의부터 하고, 빈 모델에 가중치를 덮어 씌워주는식)\n",
    "# 모델 생성 후 잘 저장하였다면 안써도 됨 결국 아래의 저장, 불러오기만 잘 하면됨.\n",
    "# from keras.models import load_model\n",
    "# model.load_weights('model/multi_tag_classification.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장(2) fit된모델이 저장됨\n",
    "model.save('model/multi_tag_classification_last.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 불러오기 fit된 모델이 불러와짐\n",
    "from keras.models import load_model\n",
    "model = load_model('model/multi_tag_classification_last.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multilabel_classifier로 val playlist를 채워보자\n",
    "## 2. val의 tag를 기준으로 songs를 100개씩 채우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_json('datas/train.json')\n",
    "# val = pd.read_json('datas/val.json')\n",
    "# song_meta = pd.read_json('datas/song_meta.json')\n",
    "# raw_df = pd.read_csv('datas/df_for_MLC.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. train + val Dataframe 생성 (predict 과정시 사용하기 위함)\n",
    "#### tag에 장르 붙이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_meta = pd.read_json('datas/song_meta.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datas = pd.concat([train, val]).reset_index(drop = True)\n",
    "ls = []\n",
    "for i in raw_datas[['tags','songs']].index:\n",
    "    tags = raw_datas.at[i,'tags']\n",
    "    songs = raw_datas.at[i,'songs']\n",
    "    for song in songs:\n",
    "        ls.append({'id':song,'tags': tags})\n",
    "song_tag = pd.DataFrame(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_tags = song_tag.groupby('id').agg(sum).reset_index()\n",
    "song_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tag의 count 붙이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_tags['tags_num'] = np.nan\n",
    "for i in range(0,len(song_tags)):\n",
    "    song_tags['tags_num'][i] = sorted(Counter(song_tags['tags'][i]).items(), key = lambda x:x[1],reverse=True)\n",
    "    song_tags['tags_num'][i] = dict(song_tags['tags_num'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = song_meta[['id', 'song_gn_gnr_basket', 'song_gn_dtl_gnr_basket']]\n",
    "song_grn = sample['song_gn_gnr_basket'] + sample['song_gn_dtl_gnr_basket']\n",
    "song = pd.DataFrame(song_grn)\n",
    "song.reset_index(inplace=True)\n",
    "song = song.rename(columns={\"index\": \"id\", 0: 'gn'})\n",
    "song"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T06:25:55.836296Z",
     "iopub.status.busy": "2020-05-28T06:25:55.836296Z",
     "iopub.status.idle": "2020-05-28T06:25:55.842272Z",
     "shell.execute_reply": "2020-05-28T06:25:55.840282Z",
     "shell.execute_reply.started": "2020-05-28T06:25:55.836296Z"
    }
   },
   "source": [
    "#### song_tags와 song을 merge (train + val의 노래, tag모두 포함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.merge(song_tags, song, left_on = 'id', right_on= 'id')\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tag에 장르 붙이기(group by)\n",
    "- 데이터가 많아서 한번에하면 오래걸려서 4번에 나누어서함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "tag_gns = []\n",
    "for i in tqdm(range(0, len(temp))):\n",
    "    for j in range(0, len(temp['tags_num'][i])):\n",
    "        tag_gns.append({'tags': list(temp['tags_num'][i].items())[j][0],\n",
    "                        'gn': temp['gn'][i] * list(temp['tags_num'][i].items())[j][1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_gns_df = pd.DataFrame(tag_gns)\n",
    "tag_gns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df의 데이터가 너무많아서 4번으로 나눠서 함(1/4)\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "empty_df = []\n",
    "for i in tqdm(range(0,len(tag_gns_df),100000)):\n",
    "    sample = tag_gns_df[i:i+100000]\n",
    "    temp = pd.DataFrame(sample['gn'].groupby(sample['tags']).sum())\n",
    "    temp.reset_index(inplace = True)\n",
    "    empty_df.append(temp)\n",
    "tag_gns_df = pd.concat(empty_df)\n",
    "tag_gns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df의 데이터가 너무많아서 4번으로 나눠서 함(2/4)\n",
    "empty_df = []\n",
    "for i in tqdm(range(0,len(tag_gns_df),100000)):\n",
    "    sample = tag_gns_df[i:i+100000]\n",
    "    temp = pd.DataFrame(sample['gn'].groupby(sample['tags']).sum())\n",
    "    temp.reset_index(inplace = True)\n",
    "    empty_df.append(temp)\n",
    "tag_gns_df = pd.concat(empty_df)\n",
    "tag_gns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df의 데이터가 너무많아서 4번으로 나눠서 함(3/4)\n",
    "empty_df = []\n",
    "for i in tqdm(range(0,len(tag_gns_df),100000)):\n",
    "    sample = tag_gns_df[i:i+100000]\n",
    "    temp = pd.DataFrame(sample['gn'].groupby(sample['tags']).sum())\n",
    "    temp.reset_index(inplace = True)\n",
    "    empty_df.append(temp)\n",
    "tag_gns_df = pd.concat(empty_df)\n",
    "tag_gns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df의 데이터가 너무많아서 4번으로 나눠서 함(4/4)\n",
    "tag_gns = pd.DataFrame(tag_gns_df['gn'].groupby(tag_gns_df['tags']).sum())\n",
    "tag_gns.reset_index(inplace = True)\n",
    "tag_gns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tag count 붙이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "tag_gns['gn_num'] = np.nan\n",
    "for i in range(0,len(tag_gns)):\n",
    "    tag_gns['gn_num'][i] = sorted(Counter(tag_gns['gn'][i]).items(), key = lambda x:x[1],reverse=True)\n",
    "    tag_gns['gn_num'][i] = dict(tag_gns['gn_num'][i])\n",
    "tag_gns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tag별 장르 구분\n",
    "- tag에 언급된 노래들의 장르 count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre = pd.read_json('./datas/genre_gn_all.json', typ = 'seriese')\n",
    "all_genre = list(genre.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이코드가 오래걸립니다. 약 1시간쯤..?\n",
    "from tqdm.notebook import tqdm\n",
    "for i in tqdm(tag_gns.index):\n",
    "    count = pd.DataFrame(list(tag_gns.loc[i]['gn_num'].items()), columns=['gn', i]).set_index('gn').T\n",
    "    tag_gns.update(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 정규화\n",
    "- tag_gns에서 필요없는 컬럼 버리고 정규화 시킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = tag_gns['tags']\n",
    "tag_gns.drop(columns = ['gn','gn_num','tags'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "MinMax_scaled = minmax_scale(tag_gns, axis=0, copy=True, )\n",
    "tag_gns_scaled = pd.DataFrame(MinMax_scaled, columns = tag_gns.columns)\n",
    "tag_gns_scaled['tags'] = temp\n",
    "tag_gns_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### df_tags를 train에 있는 tag와 val에 있는 tag로 나누어야함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 데이터\n",
    "df_tag = train['tags'].apply(lambda x : [str(i) for i in x])\n",
    "df_tag = train['tags'].reset_index(drop=True)\n",
    "unique_tas_list = np.concatenate(df_tag)\n",
    "unique_tas_list = list(set(unique_tas_list))\n",
    "unique_tas_list = [str(i) for i in unique_tas_list]\n",
    "df_tag_train = tag_gns_scaled[tag_gns_scaled['tags'].isin(unique_tas_list)]\n",
    "df_tag_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val 데이터\n",
    "df_tag = val['tags'].apply(lambda x : [str(i) for i in x])\n",
    "df_tag = val['tags'].reset_index(drop=True)\n",
    "unique_tas_list = np.concatenate(df_tag)\n",
    "unique_tas_list = list(set(unique_tas_list))\n",
    "unique_tas_list = [str(i) for i in unique_tas_list]\n",
    "df_tags_val = tag_gns_scaled[tag_gns_scaled['tags'].isin(unique_tas_list)]\n",
    "df_tags_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train에 y(song) 붙이기\n",
    "- 이것도 너무많아서 6번 나눠서함, 사실 for문 돌려도되는데.. 귀찮아요.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "ls = []\n",
    "for i in tqdm(train[['tags','songs']].index):\n",
    "    tags = train.at[i,'tags']\n",
    "    songs = train.at[i,'songs']\n",
    "    for tag in tags:\n",
    "        ls.append({'tags':tag,'songs': songs})\n",
    "tag_song = pd.DataFrame(ls)\n",
    "tag_song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_df = []\n",
    "for i in tqdm(range(0,len(tag_song),10000)):\n",
    "    sample = tag_song[i:i+10000]\n",
    "    tag_songs = pd.DataFrame(sample['songs'].groupby(sample['tags']).sum())\n",
    "    tag_songs.reset_index(inplace = True)\n",
    "    empty_df.append(tag_songs)\n",
    "df = pd.concat(empty_df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_df = []\n",
    "for i in tqdm(range(0,len(tag_song),10000)):\n",
    "    sample = df[i:i+10000]\n",
    "    tag_songs = pd.DataFrame(sample['songs'].groupby(sample['tags']).sum())\n",
    "    tag_songs.reset_index(inplace = True)\n",
    "    empty_df.append(tag_songs)\n",
    "df = pd.concat(empty_df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_df = []\n",
    "for i in tqdm(range(0,len(tag_song),10000)):\n",
    "    sample = df[i:i+10000]\n",
    "    tag_songs = pd.DataFrame(sample['songs'].groupby(sample['tags']).sum())\n",
    "    tag_songs.reset_index(inplace = True)\n",
    "    empty_df.append(tag_songs)\n",
    "df = pd.concat(empty_df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_df = []\n",
    "for i in tqdm(range(0,len(tag_song),10000)):\n",
    "    sample = df[i:i+10000]\n",
    "    tag_songs = pd.DataFrame(sample['songs'].groupby(sample['tags']).sum())\n",
    "    tag_songs.reset_index(inplace = True)\n",
    "    empty_df.append(tag_songs)\n",
    "df = pd.concat(empty_df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_songs = pd.DataFrame(df['songs'].groupby(df['tags']).sum())\n",
    "tag_songs.reset_index(inplace = True)\n",
    "tag_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y를 붙임\n",
    "df_tags_train = pd.merge(df_tag_train, tag_songs, on = 'tags')\n",
    "df_tags_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag를 인덱스로 저장\n",
    "df_tags_train.set_index('tags', inplace = True)\n",
    "df_tags_val.set_index('tags', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장\n",
    "df_tags_train.to_pickle('datas/df_tags_train.pkl')\n",
    "df_tags_val.to_pickle('datas/df_tags_val.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. keras model 생성 \n",
    "- tag -> songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T07:48:54.772656Z",
     "iopub.status.busy": "2020-05-28T07:48:54.772656Z",
     "iopub.status.idle": "2020-05-28T07:48:55.935545Z",
     "shell.execute_reply": "2020-05-28T07:48:55.935545Z",
     "shell.execute_reply.started": "2020-05-28T07:48:54.772656Z"
    }
   },
   "outputs": [],
   "source": [
    "df_tags_train = pd.read_pickle('datas/df_tags_train.pkl')\n",
    "df_tags_val = pd.read_pickle('datas/df_tags_val.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T07:48:55.936542Z",
     "iopub.status.busy": "2020-05-28T07:48:55.936542Z",
     "iopub.status.idle": "2020-05-28T07:49:02.199790Z",
     "shell.execute_reply": "2020-05-28T07:49:02.199790Z",
     "shell.execute_reply.started": "2020-05-28T07:48:55.936542Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "raw_te_tags_mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "raw_te_tags = list(df_tags_train.songs)\n",
    "y = raw_te_tags_mlb.fit_transform(raw_te_tags)\n",
    "y_classes = raw_te_tags_mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T07:49:02.200787Z",
     "iopub.status.busy": "2020-05-28T07:49:02.200787Z",
     "iopub.status.idle": "2020-05-28T07:49:02.238686Z",
     "shell.execute_reply": "2020-05-28T07:49:02.237689Z",
     "shell.execute_reply.started": "2020-05-28T07:49:02.200787Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df_tags_train[df_tags_train.columns.difference(['songs'])]\n",
    "X = X.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train의 tag에서 song을 예측하는 모델 생성\n",
    "- multi_song_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T07:50:49.284863Z",
     "iopub.status.busy": "2020-05-28T07:50:49.283844Z",
     "iopub.status.idle": "2020-05-28T07:50:49.541134Z",
     "shell.execute_reply": "2020-05-28T07:50:49.541134Z",
     "shell.execute_reply.started": "2020-05-28T07:50:49.284863Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, glob, numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import BatchNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend.tensorflow_backend as K\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "with K.tf_ops.device('/device:CPU:0'):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(32, input_shape=X.shape[1:], activation='relu'))\n",
    "    \n",
    "    model.add(Dense(64, activation='relu'))\n",
    "\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(y.shape[1], activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam')\n",
    "    \n",
    "    model_dir = './model'\n",
    "\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "\n",
    "    model_path = model_dir + '/multi_song_classification.h5'\n",
    "    checkpoint = ModelCheckpoint(filepath=model_path , monitor='loss', verbose=1, save_weights_only=True, period=5)\n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T07:50:50.245251Z",
     "iopub.status.busy": "2020-05-28T07:50:50.245251Z",
     "iopub.status.idle": "2020-05-28T07:50:55.612895Z",
     "shell.execute_reply": "2020-05-28T07:50:55.611899Z",
     "shell.execute_reply.started": "2020-05-28T07:50:50.245251Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23328 samples, validate on 5832 samples\n",
      "Epoch 1/10\n",
      "  100/23328 [..............................] - ETA: 9:22 - loss: 0.6894"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-584ec001cbb5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X, y, batch_size=100, epochs=10, validation_split = 0.2, callbacks=[checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('predict_songs_model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모델 저장(1) -> 모델의 껍데기 fit되지않은애가 저장됨.\n",
    "# model_json = model.to_json()\n",
    "# with open(\"model/multi_tag_classification.json\", \"w\") as json_file : \n",
    "#     json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델 불러오기(1) -> 모델의 껍데기 fit되지 않은애가 불러와짐\n",
    "# from keras.models import model_from_json \n",
    "# # json_file = open(\"model/multi_tag_classification.json\", \"r\") \n",
    "# loaded_model_json = json_file.read() \n",
    "# json_file.close() \n",
    "# model = model_from_json(loaded_model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 가중치 불러오기(가중치만 저장됐을떄) -> 'multi_tag_classification.h5'\n",
    "# 위의 모델 정의부터 하고, 빈 모델에 가중치를 덮어 씌워주는식)\n",
    "# 모델 생성 후 잘 저장하였다면 안써도 됨 결국 아래의 저장, 불러오기만 잘 하면됨.\n",
    "# from keras.models import load_model\n",
    "# model.load_weights('model/multi_tag_classification.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장(2)\n",
    "model.save('model/multi_song_classification_last.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 불러오기(2)\n",
    "from keras.models import load_model\n",
    "load_model = load_model('model/multi_song_classification_last.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
